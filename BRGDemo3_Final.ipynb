{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-experimental langchain-community langchain networkx langchain-google-genai langchain-core json-repair tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ruWbyS15Gek",
        "outputId": "3f4e3ef0-384d-4929-b33a-b6697f5a2d57"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.0.9)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.32)\n",
            "Requirement already satisfied: json-repair in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.8.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.155.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.25.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.67.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.26.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.66.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import os\n",
        "\n",
        "def parse_rules_file(rules_file_path):\n",
        "    \"\"\"\n",
        "    Parses a Suricata rules file and extracts relevant information\n",
        "    :param rules_file_path: Path to the .rules file\n",
        "    :return: A list of dictionaries with extracted rule information\n",
        "    \"\"\"\n",
        "    extracted_data = []\n",
        "\n",
        "    # Regex pattern to extract msg, classtype, and sid\n",
        "    rule_pattern = re.compile(\n",
        "        r'msg:\"(?P<msg>.*?)\";.*?classtype:(?P<classtype>[^;]+);.*?sid:(?P<sid>\\d+);'\n",
        "    )\n",
        "\n",
        "    with open(rules_file_path, 'r', encoding='utf-8') as file:\n",
        "        for line_number, line in enumerate(file, 1):\n",
        "            line = line.strip()\n",
        "            # Skip comments/empty lines\n",
        "            if not line or line.startswith('#'):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                match = rule_pattern.search(line)\n",
        "                if match:\n",
        "                    suri_rule_msg = match.group('msg')\n",
        "                    suri_rule_classtype = match.group('classtype')\n",
        "                    suri_rule_id = match.group('sid')\n",
        "\n",
        "                    extracted_data.append({\n",
        "                        \"suri_rule_id\": suri_rule_id,\n",
        "                        \"suri_rule_classtype\": suri_rule_classtype,\n",
        "                        \"suri_rule_msg\": suri_rule_msg\n",
        "                    })\n",
        "                else:\n",
        "                    print(f\"Warning: Line {line_number} in '{rules_file_path}' \"\n",
        "                          f\"does not match expected format.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing line {line_number} in '{rules_file_path}': {e}\")\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "\n",
        "def parse_all_rules_in_directory(rules_folder_path):\n",
        "    \"\"\"\n",
        "    Parses all .rules files within the specified folder and aggregates the data.\n",
        "    :param rules_folder_path: Path to the folder containing .rules files\n",
        "    :return: A list of extracted rule dictionaries (with file_name included)\n",
        "    \"\"\"\n",
        "    all_extracted_rules = []\n",
        "\n",
        "    # List all files in the folder\n",
        "    for filename in os.listdir(rules_folder_path):\n",
        "        if filename.endswith('.rules'):\n",
        "            rules_file_path = os.path.join(rules_folder_path, filename)\n",
        "            print(f\"Parsing rules from file: {rules_file_path}\")\n",
        "\n",
        "            file_rules = parse_rules_file(rules_file_path)\n",
        "            # Tag each rule with the file_name\n",
        "            for rule in file_rules:\n",
        "                rule[\"file_name\"] = filename\n",
        "\n",
        "            all_extracted_rules.extend(file_rules)\n",
        "\n",
        "    return all_extracted_rules\n",
        "\n",
        "\n",
        "def save_to_json(data, output_file_path):\n",
        "    \"\"\"\n",
        "    Saves extracted data to a JSON file\n",
        "    :param data: List of extracted rule data\n",
        "    :param output_file_path: Path to the output JSON file\n",
        "    \"\"\"\n",
        "    with open(output_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "    print(f\"\\nExtracted rules saved to: {output_file_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Folder containing the .rules files\n",
        "    rules_folder = \"rules\"  # or \"/content/rules\" if in Google Colab, etc.\n",
        "\n",
        "    # Output JSON file path\n",
        "    output_file_path = \"testData.json\"\n",
        "\n",
        "    # Parse all .rules files in the folder\n",
        "    print(f\"Scanning for .rules files in: {rules_folder}\")\n",
        "    extracted_rules = parse_all_rules_in_directory(rules_folder)\n",
        "\n",
        "    # Save to JSON\n",
        "    save_to_json(extracted_rules, output_file_path)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Total .rules files processed: \"\n",
        "          f\"{len([f for f in os.listdir(rules_folder) if f.endswith('.rules')])}\")\n",
        "    print(f\"Total rules extracted: {len(extracted_rules)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "caOc_0sZSCwj",
        "outputId": "99e92a88-a0ff-4a96-a491-ef6f15b291b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning for .rules files in: rules\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'rules'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5c572fdf8652>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Parse all .rules files in the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Scanning for .rules files in: {rules_folder}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mextracted_rules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_all_rules_in_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrules_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Save to JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5c572fdf8652>\u001b[0m in \u001b[0;36mparse_all_rules_in_directory\u001b[0;34m(rules_folder_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# List all files in the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrules_folder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.rules'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mrules_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrules_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rules'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set OpenAI API key directly\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = ''"
      ],
      "metadata": {
        "id": "rDfpNfJOWHMm"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL=\"\" #p[inecone url]\n",
        "import os\n",
        "\n",
        "# Replace these strings with your actual API keys\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"\"\n",
        "os.environ[\"OPENAI_KEY\"] = \"\"\n",
        "# Optionally print to verify they are set (optional, remove in production)\n",
        "print(\"PINECONE_API_KEY:\", os.environ.get(\"PINECONE_API_KEY\", \"Not found\"))\n",
        "print(\"OPENAI_KEY:\", os.environ.get(\"OPENAI_KEY\", \"Not found\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu5gOqTrjoko",
        "outputId": "865c205f-dcf2-47f3-d7c3-c7737782424a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PINECONE_API_KEY: pcsk_4iJy6B_4jDRvxB4cU9yVd8LHkZNZmqEMwoKkDVdzPEzuPycBfJDpvgbek6TBGKiozbjDwq\n",
            "OPENAI_KEY: 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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client langchain_community unittest\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COrTYwcTjogn",
        "outputId": "07bcf93e-a5d0-42c3-9ae7-fd96da57ff91"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.11/dist-packages (5.0.1)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement unittest (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for unittest\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXScl80UjoeI",
        "outputId": "1dff705e-e04e-4d53-85e7-cc63d9f76c80"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.16 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.16)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.32 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.32)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain_community) (0.3.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain_community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken pinecone openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2scVrJvpjobP",
        "outputId": "9ebed179-2e3c-47a0-d53c-d2f02fcafa5f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.11/dist-packages (5.4.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.9)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2024.12.14)\n",
            "Requirement already satisfied: pinecone-plugin-inference<4.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (3.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import time\n",
        "\n",
        "class TestExperienceManagerAndVectorDB(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.project_name = \"testautoattackerproject\"  # Must be lower-case, no special chars\n",
        "        self.vectordb_name = \"test_autoattacker_vectordb\"\n",
        "        # self.experience_manager = ExperienceManager(project_name=self.project_name,\n",
        "        #                                             vectordb_name=self.vectordb_name)\n",
        "        self.vector_db = customVectorDB(project_name=self.project_name,\n",
        "                                        vectordb_name=self.vectordb_name)\n",
        "\n",
        "    def tearDown(self):\n",
        "        # Optionally delete the index after tests\n",
        "        self.vector_db.delete_index()\n",
        "        pass\n",
        "\n",
        "    def test_store_and_retrieve_text(self):\n",
        "        test_text = \"This is a test text for vector database.\"\n",
        "        self.vector_db.store_text(test_text)\n",
        "        time.sleep(5)  # Wait for indexing\n",
        "        results = self.vector_db.retrieval(\"test text\")\n",
        "        print(\"Stored Text Retrieval Results:\", results)\n",
        "        self.assertGreater(len(results), 0)\n",
        "        if results:\n",
        "            self.assertIn(\"This is a test text for vector database.\", results[0].page_content)\n",
        "\n",
        "    def test_store_and_retrieve_file(self):\n",
        "        test_file_content = \"This is a test file content for vector database.\"\n",
        "        with open(\"test_file.txt\", \"w\") as f:\n",
        "            f.write(test_file_content)\n",
        "\n",
        "        self.vector_db.store_file(\"test_file.txt\")\n",
        "        time.sleep(5)\n",
        "        results = self.vector_db.retrieval(\"test file content\")\n",
        "        print(\"Stored File Retrieval Results:\", results)\n",
        "        self.assertGreater(len(results), 0)\n",
        "        if results:\n",
        "            self.assertIn(\"This is a test file content for vector database.\", results[0].page_content)\n",
        "\n",
        "    # def test_store_and_retrieve_experience(self):\n",
        "    #     test_action_plan = \"Execute shell command to escalate privileges.\"\n",
        "    #     self.experience_manager.store_experience(test_action_plan, metadata=None)\n",
        "    #     time.sleep(5)\n",
        "    #     results = self.experience_manager.retrieve_experiences(\"escalate privileges\", top_k=1)\n",
        "    #     print(\"Stored Experience Retrieval Results:\", results)\n",
        "    #     self.assertGreater(len(results), 0)\n",
        "    #     if results:\n",
        "    #         self.assertIn(\"Execute shell command to escalate privileges.\", results[0].page_content)\n",
        "\n",
        "\n",
        "# Run the tests\n",
        "suite = unittest.TestLoader().loadTestsFromTestCase(TestExperienceManagerAndVectorDB)\n",
        "unittest.TextTestRunner(verbosity=2).run(suite)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jEsvSJdjoXE",
        "outputId": "3f81bc04-236c-4cd8-9799-ff1c075f19f9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_store_and_retrieve_file (__main__.TestExperienceManagerAndVectorDB.test_store_and_retrieve_file) ... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stored File Retrieval Results: [Document(metadata={}, page_content='This is a test file content for vector database.'), Document(metadata={'technique_id': 'T1595', 'technique_name': 'Active Scanning'}, page_content='Technique ID: T1595\\nTechnique Name: Active Scanning\\n\\nAdversaries may execute active reconnaissance scans to gather information that can be used during targeting. Active scans are those where the adversary probes victim infrastructure via network traffic, as opposed to other forms of reconnaissance that do not involve direct interaction.'), Document(metadata={'technique_id': 'T1595', 'technique_name': 'Active Scanning'}, page_content='Technique ID: T1595\\nTechnique Name: Active Scanning\\n\\nAdversaries may execute active reconnaissance scans to gather information that can be used during targeting. Active scans are those where the adversary probes victim infrastructure via network traffic, as opposed to other forms of reconnaissance that do not involve direct interaction.'), Document(metadata={'technique_id': 'T1595', 'technique_name': 'Active Scanning'}, page_content='Technique ID: T1595\\nTechnique Name: Active Scanning\\n\\nAdversaries may execute active reconnaissance scans to gather information that can be used during targeting. Active scans are those where the adversary probes victim infrastructure via network traffic, as opposed to other forms of reconnaissance that do not involve direct interaction.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ok\n",
            "test_store_and_retrieve_text (__main__.TestExperienceManagerAndVectorDB.test_store_and_retrieve_text) ... FAIL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stored Text Retrieval Results: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "======================================================================\n",
            "FAIL: test_store_and_retrieve_text (__main__.TestExperienceManagerAndVectorDB.test_store_and_retrieve_text)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-46-4154f70b3f34>\", line 25, in test_store_and_retrieve_text\n",
            "    self.assertGreater(len(results), 0)\n",
            "AssertionError: 0 not greater than 0\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 52.006s\n",
            "\n",
            "FAILED (failures=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=2 errors=0 failures=1>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import uuid\n",
        "import pinecone\n",
        "import time\n",
        "import json\n",
        "from typing import List\n",
        "\n",
        "# Install necessary packages (if not already installed)\n",
        "!pip install pinecone-client langchain_community\n",
        "\n",
        "# Set environment variables (replace with your actual keys)\n",
        "# os.environ[\"PINECONE_API_KEY\"] = \"YOUR_PINECONE_API_KEY\"\n",
        "# os.environ[\"OPENAI_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "# LangChain-related imports\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "\n",
        "from pinecone import Pinecone as PineconeClient, ServerlessSpec\n",
        "\n",
        "\n",
        "class customVectorDB:\n",
        "    \"\"\"\n",
        "    The custom VectorDB implementation behind Pinecone to support the chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, project_name: str, vectordb_name: str):\n",
        "        assert project_name != \"\", \"Project name cannot be empty.\"\n",
        "        self.project_name = project_name\n",
        "\n",
        "        # Load environment variables\n",
        "        pinecone_api_key = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
        "        os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_KEY\", \"\")\n",
        "\n",
        "        # In Colab, create a local directory if you wish.\n",
        "        self.vectordb_directory = vectordb_name\n",
        "        if not os.path.exists(self.vectordb_directory):\n",
        "            os.mkdir(self.vectordb_directory)\n",
        "\n",
        "        self.uuid = str(uuid.uuid4())\n",
        "        self.local_context_directory = os.path.join(\n",
        "            self.vectordb_directory, self.project_name + \"_\" + self.uuid\n",
        "        )\n",
        "        if not os.path.exists(self.local_context_directory):\n",
        "            os.mkdir(self.local_context_directory)\n",
        "\n",
        "        # Initialize Pinecone\n",
        "        self.pinecone_instance = PineconeClient(api_key=pinecone_api_key)\n",
        "        existing_indexes = self.pinecone_instance.list_indexes().names()\n",
        "\n",
        "        if self.project_name not in existing_indexes:\n",
        "            self.pinecone_instance.create_index(\n",
        "                name=self.project_name,\n",
        "                dimension=1536,  # 'text-embedding-ada-002' uses 1536 dimensions\n",
        "                metric=\"cosine\",\n",
        "                spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "            )\n",
        "\n",
        "        self.vectorDB = Pinecone.from_existing_index(\n",
        "            self.project_name,\n",
        "            OpenAIEmbeddings()\n",
        "        )\n",
        "\n",
        "    def __del__(self):\n",
        "        pass\n",
        "\n",
        "    def _save_text(self, _text: str) -> str:\n",
        "        filename = str(uuid.uuid4()) + \".txt\"\n",
        "        file_path = os.path.join(self.local_context_directory, filename)\n",
        "        with open(file_path, \"w\") as f:\n",
        "            f.write(_text)\n",
        "        return file_path\n",
        "\n",
        "    def store_file(self, filename: str, metadata: List[dict] = None):\n",
        "        loader = TextLoader(filename)\n",
        "        documents = loader.load()\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "        string_texts = [t.page_content for t in texts]\n",
        "        self.vectorDB.add_texts(\n",
        "            texts=string_texts,\n",
        "            metadatas=metadata if metadata else None\n",
        "        )\n",
        "\n",
        "    def store_text(self, content: str, metadata: List[dict] = None):\n",
        "        filename = self._save_text(content)\n",
        "        self.store_file(filename, metadata=metadata)\n",
        "\n",
        "    def retrieval(self, keyword: str, metadata: List[dict] = None) -> List[dict]:\n",
        "        return self.vectorDB.similarity_search(keyword)\n",
        "\n",
        "    def delete_index(self):\n",
        "        self.pinecone_instance.delete_index(name=self.project_name)\n",
        "\n",
        "\n",
        "class ExperienceManager:\n",
        "    def __init__(self, project_name: str, vectordb_name: str):\n",
        "        self.vector_db = customVectorDB(project_name, vectordb_name)\n",
        "\n",
        "    def store_experience(self, action_plan: str, metadata: List[dict] = None):\n",
        "        self.vector_db.store_text(action_plan, metadata=metadata)\n",
        "\n",
        "    def retrieve_experiences(self, query: str, top_k: int = 3) -> List[dict]:\n",
        "        results = self.vector_db.retrieval(query)\n",
        "        return results[:top_k]\n",
        "\n",
        "\n",
        "# Instantiate your customVectorDB and ExperienceManager\n",
        "project_name = \"testautoattackerproject\"\n",
        "vectordb_name = \"test_autoattacker_vectorDB\"\n",
        "\n",
        "vector_db = customVectorDB(project_name, vectordb_name)  # if you want direct access\n",
        "manager = ExperienceManager(project_name, vectordb_name) # if you prefer the manager\n",
        "json_file_path = \"techniques.json\"  # The file you uploaded\n",
        "push_techniques_to_experience_manager(json_file_path, manager)\n",
        "\n",
        "# Wait a few seconds for indexing if you want to test retrieval immediately\n",
        "time.sleep(5)\n",
        "\n",
        "# Example retrieval\n",
        "query = \"Password Filter DLL\"\n",
        "results = manager.retrieve_experiences(query)\n",
        "print(f\"Results for query: '{query}'\")\n",
        "for idx, doc in enumerate(results):\n",
        "    print(f\"\\nResult {idx+1} Content:\\n\", doc.page_content)\n",
        "    print(\"Metadata:\", doc.metadata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfkpkr5Pj5CY",
        "outputId": "fb804319-982a-43ca-880f-a78547fbae57"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.11/dist-packages (5.0.1)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2024.12.14)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.16 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.16)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.32 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.32)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain_community) (0.3.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain_community) (24.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain_community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Results for query: 'Password Filter DLL'\n",
            "\n",
            "Result 1 Content:\n",
            " Technique ID: T1201\n",
            "Technique Name: Password Policy Discovery\n",
            "\n",
            "Adversaries may attempt to access detailed information about the password policy used within an enterprise network or cloud environment. Password policies are a way to enforce complex passwords that are difficult to guess or crack through Brute Force. This information may help the adversary to create a list of common passwords and launch dictionary and/or brute force attacks which adheres to the policy (e.g. if the minimum password length should be 8, then not trying passwords such as 'pass123'; not checking for more than 3-4 passwords per account if the lockout is set to 6 as to not lock out accounts).\n",
            "Metadata: {'technique_id': 'T1201', 'technique_name': 'Password Policy Discovery'}\n",
            "\n",
            "Result 2 Content:\n",
            " Technique ID: T1555\n",
            "Technique Name: Credentials from Password Stores\n",
            "\n",
            "Adversaries may search for common password storage locations to obtain user credentials. Passwords are stored in several places on a system, depending on the operating system or application holding the credentials. There are also specific applications and services that store passwords to make them easier for users to manage and maintain, such as password managers and cloud secrets vaults. Once credentials are obtained, they can be used to perform lateral movement and access restricted information.\n",
            "Metadata: {'technique_id': 'T1555', 'technique_name': 'Credentials from Password Stores'}\n",
            "\n",
            "Result 3 Content:\n",
            " Technique ID: T1556\n",
            "Technique Name: Modify Authentication Process\n",
            "\n",
            "Adversaries may modify authentication mechanisms and processes to access user credentials or enable otherwise unwarranted access to accounts. The authentication process is handled by mechanisms, such as the Local Security Authentication Server (LSASS) process and the Security Accounts Manager (SAM) on Windows, pluggable authentication modules (PAM) on Unix-based systems, and authorization plugins on MacOS systems, responsible for gathering, storing, and validating credentials. By modifying an authentication process, an adversary may be able to authenticate to a service or system without using Valid Accounts.\n",
            "Metadata: {'technique_id': 'T1556', 'technique_name': 'Modify Authentication Process'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Password Filter DLL\"\n",
        "results = manager.retrieve_experiences(query)\n",
        "for idx, doc in enumerate(results):\n",
        "    print(f\"\\nResult {idx+1} Content:\\n\", doc.page_content)\n",
        "    print(\"Metadata:\", doc.metadata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8RMb_SpoOlc",
        "outputId": "97588b1e-3937-45e1-d232-d3aa27ab3440"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Result 1 Content:\n",
            " Technique ID: T1201\n",
            "Technique Name: Password Policy Discovery\n",
            "\n",
            "Adversaries may attempt to access detailed information about the password policy used within an enterprise network or cloud environment. Password policies are a way to enforce complex passwords that are difficult to guess or crack through Brute Force. This information may help the adversary to create a list of common passwords and launch dictionary and/or brute force attacks which adheres to the policy (e.g. if the minimum password length should be 8, then not trying passwords such as 'pass123'; not checking for more than 3-4 passwords per account if the lockout is set to 6 as to not lock out accounts).\n",
            "Metadata: {'technique_id': 'T1201', 'technique_name': 'Password Policy Discovery'}\n",
            "\n",
            "Result 2 Content:\n",
            " Technique ID: T1555\n",
            "Technique Name: Credentials from Password Stores\n",
            "\n",
            "Adversaries may search for common password storage locations to obtain user credentials. Passwords are stored in several places on a system, depending on the operating system or application holding the credentials. There are also specific applications and services that store passwords to make them easier for users to manage and maintain, such as password managers and cloud secrets vaults. Once credentials are obtained, they can be used to perform lateral movement and access restricted information.\n",
            "Metadata: {'technique_id': 'T1555', 'technique_name': 'Credentials from Password Stores'}\n",
            "\n",
            "Result 3 Content:\n",
            " Technique ID: T1556\n",
            "Technique Name: Modify Authentication Process\n",
            "\n",
            "Adversaries may modify authentication mechanisms and processes to access user credentials or enable otherwise unwarranted access to accounts. The authentication process is handled by mechanisms, such as the Local Security Authentication Server (LSASS) process and the Security Accounts Manager (SAM) on Windows, pluggable authentication modules (PAM) on Unix-based systems, and authorization plugins on MacOS systems, responsible for gathering, storing, and validating credentials. By modifying an authentication process, an adversary may be able to authenticate to a service or system without using Valid Accounts.\n",
            "Metadata: {'technique_id': 'T1556', 'technique_name': 'Modify Authentication Process'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def push_techniques_to_experience_manager(json_file_path: str, manager: ExperienceManager):\n",
        "    with open(json_file_path, \"r\") as f:\n",
        "        techniques = json.load(f)\n",
        "\n",
        "    for technique in techniques:\n",
        "        # Combine fields into a single text block\n",
        "        text_content = (\n",
        "            f\"Technique ID: {technique.get('technique_id', '')}\\n\"\n",
        "            f\"Technique Name: {technique.get('technique_name', '')}\\n\\n\"\n",
        "            f\"{technique.get('technique_description', '')}\"\n",
        "        )\n",
        "\n",
        "        # Optional: store some metadata as a list of dict or just a single dict\n",
        "        metadata = [{\n",
        "            \"technique_id\": technique.get(\"technique_id\", \"\"),\n",
        "            \"technique_name\": technique.get(\"technique_name\", \"\"),\n",
        "        }]\n",
        "\n",
        "        # Store via ExperienceManager\n",
        "        manager.store_experience(text_content, metadata=metadata)\n"
      ],
      "metadata": {
        "id": "t_2O8EW8joNH"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L5gWvLcckD8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I get the .csv forom suricaa_Rule enhancer.py script and then uplaod that here.\n",
        "#upload the MITRE Techniques.json file as well techniques.json\n",
        "#In case the technique ID is not present in the Hashmap it can requery the llm."
      ],
      "metadata": {
        "id": "pKu0MmaGv9tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# import os\n",
        "# import csv\n",
        "# import re\n",
        "# import openai\n",
        "# from collections import OrderedDict\n",
        "# from google.colab import files  # Google Colab auto-download\n",
        "\n",
        "# # OpenAI Client\n",
        "# client = openai\n",
        "# client.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# # Cache to avoid redundant API calls\n",
        "# cache = {}\n",
        "\n",
        "# # Regex for extracting msg and classtype\n",
        "# MSG_REGEX = re.compile(r'msg:\"([^\"]+)\"', re.IGNORECASE)\n",
        "# CLASSTYPE_REGEX = re.compile(r'classtype:([^;]+);', re.IGNORECASE)\n",
        "\n",
        "# # Constants\n",
        "# BATCH_SIZE = 2000\n",
        "# SAVE_INTERVAL = 1000\n",
        "# OUTPUT_DIR = \"output_batches\"\n",
        "# TECHNIQUES_JSON_FILE = \"techniques.json\"\n",
        "# TEST_MODE = False\n",
        "\n",
        "# # Ensure output directory exists\n",
        "# if not os.path.exists(OUTPUT_DIR):\n",
        "#     os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# # Load MITRE ATT&CK Techniques\n",
        "# def load_mitre_techniques():\n",
        "#     with open(TECHNIQUES_JSON_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "#         techniques = json.load(f)\n",
        "#     return {t[\"technique_id\"] for t in techniques}  # Store only technique IDs for fast lookup\n",
        "\n",
        "# MITRE_TECHNIQUES = load_mitre_techniques()\n",
        "\n",
        "# # Read processed rule IDs from existing JSON files\n",
        "# def get_processed_rule_ids():\n",
        "#     processed_ids = set()\n",
        "#     for filename in os.listdir(OUTPUT_DIR):\n",
        "#         if filename.endswith(\".json\"):\n",
        "#             with open(os.path.join(OUTPUT_DIR, filename), \"r\", encoding=\"utf-8\") as f:\n",
        "#                 try:\n",
        "#                     data = json.load(f)\n",
        "#                     processed_ids.update(entry[\"suri_rule_id\"] for entry in data)\n",
        "#                 except json.JSONDecodeError:\n",
        "#                     print(f\" Warning: Corrupted JSON file {filename}. Skipping.\")\n",
        "#     return processed_ids\n",
        "\n",
        "# # Create LLM prompt for single classification\n",
        "# def create_prompt_for_single_classification(rule):\n",
        "#     return f\"\"\"\n",
        "# Map the following Suricata IDS rule to a **single MITRE ATT&CK technique** with a confidence score (rounded to 2 decimal places).\n",
        "\n",
        "# ### Rule:\n",
        "# ID: {rule[\"suri_rule_id\"]}\n",
        "# File Name: {rule[\"file_name\"]}\n",
        "# Action: {rule[\"action\"]}\n",
        "# Protocol: {rule[\"protocol\"]}\n",
        "# Source: {rule[\"src_addr\"]}:{rule[\"src_port\"]}\n",
        "# Destination: {rule[\"dst_addr\"]}:{rule[\"dst_port\"]}\n",
        "# Options: {rule[\"options\"]}\n",
        "# Classification: {rule[\"suri_rule_classtype\"]}\n",
        "# Message: \"{rule[\"suri_rule_msg\"]}\"\n",
        "\n",
        "# Respond in valid JSON format:\n",
        "# {{\n",
        "#   \"mitre_technique_id\": \"<Technique ID>\",\n",
        "#   \"mitre_technique_name\": \"<Technique Name>\",\n",
        "#   \"confidence_score\": \"<Confidence Score (rounded to 2 decimal places)>\"\n",
        "# }}\n",
        "# \"\"\".strip()\n",
        "\n",
        "# # Query the LLM\n",
        "# def query_llm(prompt, use_gpt4=False):\n",
        "#     model_name = \"gpt-4\" if use_gpt4 else \"gpt-3.5-turbo\"\n",
        "#     system_message = (\n",
        "#         \"You are a cybersecurity expert. Your task is to map Suricata IDS rules \"\n",
        "#         \"to MITRE ATT&CK techniques, providing a confidence score (rounded to 2 decimal places).\"\n",
        "#     )\n",
        "\n",
        "#     response = client.chat.completions.create(\n",
        "#         model=model_name,\n",
        "#         messages=[\n",
        "#             {\"role\": \"system\", \"content\": system_message},\n",
        "#             {\"role\": \"user\", \"content\": prompt},\n",
        "#         ],\n",
        "#         max_tokens=400,\n",
        "#         temperature=0.0\n",
        "#     )\n",
        "#     return response\n",
        "\n",
        "# # Validate technique, if invalid re-query LLM\n",
        "# def validate_or_requery_technique(rule, mapping, use_gpt4=False):\n",
        "#     \"\"\"\n",
        "#     If the technique ID is not found in MITRE_TECHNIQUES, we **re-query the LLM** with the same prompt.\n",
        "#     \"\"\"\n",
        "#     if mapping[\"mitre_technique_id\"] in MITRE_TECHNIQUES:\n",
        "#         return mapping  # If it's valid, return it as is.\n",
        "\n",
        "#     print(f\" Invalid MITRE Technique: {mapping['mitre_technique_id']}  Re-querying LLM...\")\n",
        "\n",
        "#     # Re-run the same LLM query to get a better technique match\n",
        "#     prompt = create_prompt_for_single_classification(rule)\n",
        "#     response = query_llm(prompt, use_gpt4=use_gpt4)\n",
        "#     response_text = response.choices[0].message.content.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "#     try:\n",
        "#         corrected_mapping = json.loads(response_text)\n",
        "#         return corrected_mapping  # Return the corrected technique\n",
        "#     except json.JSONDecodeError:\n",
        "#         print(f\" Error: Unable to parse corrected technique for rule {rule['suri_rule_id']}. Skipping...\")\n",
        "#         return None  # Skip this rule if re-querying fails.\n",
        "\n",
        "# # Process CSV and map rules\n",
        "# def process_csv_and_map_to_mitre(csv_input_file, use_gpt4=False):\n",
        "#     processed_rule_ids = get_processed_rule_ids()\n",
        "#     rules = []\n",
        "\n",
        "#     with open(csv_input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "#         reader = csv.DictReader(f)\n",
        "#         for i, row in enumerate(reader, start=1):\n",
        "#             suri_rule_id = f\"{row['file_name']}_{i}\"\n",
        "#             if suri_rule_id in processed_rule_ids:\n",
        "#                 continue\n",
        "\n",
        "#             options = row[\"options\"]\n",
        "#             msg_match = MSG_REGEX.search(options)\n",
        "#             suri_rule_msg = msg_match.group(1).strip() if msg_match else \"(no msg found)\"\n",
        "\n",
        "#             classtype_match = CLASSTYPE_REGEX.search(options)\n",
        "#             suri_rule_classtype = classtype_match.group(1).strip() if classtype_match else \"(no classtype found)\"\n",
        "\n",
        "#             rule_dict = {\n",
        "#                 \"suri_rule_id\": suri_rule_id,\n",
        "#                 \"suri_rule_classtype\": suri_rule_classtype,\n",
        "#                 \"suri_rule_msg\": suri_rule_msg,\n",
        "#                 \"file_name\": row[\"file_name\"],\n",
        "#                 \"action\": row[\"action\"],\n",
        "#                 \"protocol\": row[\"protocol\"],\n",
        "#                 \"src_addr\": row[\"src_addr\"],\n",
        "#                 \"src_port\": row[\"src_port\"],\n",
        "#                 \"dst_addr\": row[\"dst_addr\"],\n",
        "#                 \"dst_port\": row[\"dst_port\"],\n",
        "#                 \"options\": options\n",
        "#             }\n",
        "#             rules.append(rule_dict)\n",
        "\n",
        "#     if TEST_MODE:\n",
        "#         rules = rules[:10]\n",
        "\n",
        "#     batch_count = 0\n",
        "#     processed_results = []\n",
        "\n",
        "#     for idx, rule in enumerate(rules, start=1):\n",
        "#         prompt = create_prompt_for_single_classification(rule)\n",
        "#         response = query_llm(prompt, use_gpt4=use_gpt4)\n",
        "#         response_text = response.choices[0].message.content.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "#         try:\n",
        "#             mapping = json.loads(response_text)\n",
        "#             valid_mapping = validate_or_requery_technique(rule, mapping, use_gpt4)\n",
        "\n",
        "#             if valid_mapping:\n",
        "#               processed_results.append({\n",
        "#                   \"suri_rule_id\": rule[\"suri_rule_id\"],\n",
        "#                   \"suri_rule_classtype\": rule[\"suri_rule_classtype\"],\n",
        "#                   \"suri_rule_msg\": rule[\"suri_rule_msg\"],\n",
        "#                   \"file_name\": rule[\"file_name\"],\n",
        "#                   \"action\": rule[\"action\"],\n",
        "#                   \"protocol\": rule[\"protocol\"],\n",
        "#                   \"src_addr\": rule[\"src_addr\"],\n",
        "#                   \"src_port\": rule[\"src_port\"],\n",
        "#                   \"dst_addr\": rule[\"dst_addr\"],\n",
        "#                   \"dst_port\": rule[\"dst_port\"],\n",
        "#                   \"options\": rule[\"options\"],\n",
        "\n",
        "#                   # Mapped MITRE Technique from LLM\n",
        "#                   \"mitre_technique_id\": valid_mapping[\"mitre_technique_id\"],\n",
        "#                   \"mitre_technique_name\": valid_mapping[\"mitre_technique_name\"],\n",
        "#                   \"confidence_score\": valid_mapping[\"confidence_score\"]\n",
        "#               })\n",
        "#         except json.JSONDecodeError:\n",
        "#             print(f\" Skipping rule {rule['suri_rule_id']} due to invalid LLM response.\")\n",
        "\n",
        "#         if idx % SAVE_INTERVAL == 0 or idx == len(rules):\n",
        "#             batch_count += 1\n",
        "#             batch_output_file = os.path.join(OUTPUT_DIR, f\"mapped_results_batch_{batch_count}.json\")\n",
        "\n",
        "#             with open(batch_output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
        "#                 json.dump(processed_results, f_out, indent=4)\n",
        "\n",
        "#             print(f\" Saved batch {batch_count} ({idx} rules) to {batch_output_file}\")\n",
        "#             files.download(batch_output_file)\n",
        "#             processed_results = []\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     process_csv_and_map_to_mitre(\"suricata_extracted_rules_parsed.csv\", use_gpt4=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S5ITSEyYgVUX",
        "outputId": "d27d5cab-91cb-4c41-db91-3695b85da4bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1107  Re-querying LLM...\n",
            " Saved batch 1 (1000 rules) to output_batches/mapped_results_batch_1.json\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_5eb66af6-56af-430e-998e-cc4a85bc18b1\", \"mapped_results_batch_1.json\", 1382470)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Saved batch 2 (2000 rules) to output_batches/mapped_results_batch_2.json\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_60b4f162-b8df-49dd-b8fc-d702b09e7032\", \"mapped_results_batch_2.json\", 1397800)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.11/typing.py:409: RuntimeWarning: coroutine 'process_rules' was never awaited\n",
            "  ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Saved batch 3 (3000 rules) to output_batches/mapped_results_batch_3.json\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_ca525f20-df40-47db-b281-2c80ac92f09f\", \"mapped_results_batch_3.json\", 1416930)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1500  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1506  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1506  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1506  Re-querying LLM...\n",
            " Saved batch 4 (4000 rules) to output_batches/mapped_results_batch_4.json\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_06e38599-5b2c-4e4d-83a6-e11098323771\", \"mapped_results_batch_4.json\", 1409968)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1107  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1100  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1506  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1059.003  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1059.001  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1192  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1192  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1024  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Saved batch 5 (5000 rules) to output_batches/mapped_results_batch_5.json\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_9cc211dc-f889-4689-98f8-cd2e728d58da\", \"mapped_results_batch_5.json\", 1402038)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Invalid MITRE Technique: T1073  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1022  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1024  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1024  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1022  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1024  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1193  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1107  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1024  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1192  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1116  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1116  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1192  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1024  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1024  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1193  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1059.003  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1059.003  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1059.001  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1059.001  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1086  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1022  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1022  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1022  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-d9807f7a77ec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mprocess_csv_and_map_to_mitre\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"suricata_extracted_rules_parsed.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpt4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-d9807f7a77ec>\u001b[0m in \u001b[0;36mprocess_csv_and_map_to_mitre\u001b[0;34m(csv_input_file, use_gpt4)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mvalid_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_or_requery_technique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpt4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalid_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-d9807f7a77ec>\u001b[0m in \u001b[0;36mvalidate_or_requery_technique\u001b[0;34m(rule, mapping, use_gpt4)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# Re-run the same LLM query to get a better technique match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_prompt_for_single_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpt4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpt4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mresponse_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"```json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"```\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-d9807f7a77ec>\u001b[0m in \u001b[0;36mquery_llm\u001b[0;34m(prompt, use_gpt4)\u001b[0m\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    857\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    860\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    997\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# import os\n",
        "# import csv\n",
        "# import re\n",
        "# import openai\n",
        "# from google.colab import files  # Google Colab auto-download\n",
        "\n",
        "# # OpenAI Client\n",
        "# client = openai\n",
        "# client.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# # Cache to avoid redundant API calls\n",
        "# cache = {}\n",
        "\n",
        "# # Regex for extracting msg and classtype\n",
        "# MSG_REGEX = re.compile(r'msg:\"([^\"]+)\"', re.IGNORECASE)\n",
        "# CLASSTYPE_REGEX = re.compile(r'classtype:([^;]+);', re.IGNORECASE)\n",
        "\n",
        "# # Constants\n",
        "# BATCH_SIZE = 2000\n",
        "# SAVE_INTERVAL = 1000\n",
        "# PROGRESS_UPDATE_INTERVAL = 50\n",
        "# START_FROM_RULE = 5001\n",
        "# OUTPUT_DIR = \"output_batches\"\n",
        "# TECHNIQUES_JSON_FILE = \"techniques.json\"\n",
        "# TEST_MODE = False\n",
        "\n",
        "# # Ensure output directory exists\n",
        "# if not os.path.exists(OUTPUT_DIR):\n",
        "#     os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# # Load MITRE ATT&CK Techniques\n",
        "# def load_mitre_techniques():\n",
        "#     with open(TECHNIQUES_JSON_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "#         techniques = json.load(f)\n",
        "#     return {t[\"technique_id\"]: t for t in techniques}  # Store technique details\n",
        "\n",
        "# MITRE_TECHNIQUES = load_mitre_techniques()\n",
        "\n",
        "# # Read processed rule IDs from existing JSON files\n",
        "# def get_processed_rule_ids():\n",
        "#     processed_ids = set()\n",
        "#     for filename in os.listdir(OUTPUT_DIR):\n",
        "#         if filename.endswith(\".json\"):\n",
        "#             with open(os.path.join(OUTPUT_DIR, filename), \"r\", encoding=\"utf-8\") as f:\n",
        "#                 try:\n",
        "#                     data = json.load(f)\n",
        "#                     processed_ids.update(entry[\"suri_rule_id\"] for entry in data)\n",
        "#                 except json.JSONDecodeError:\n",
        "#                     print(f\" Warning: Corrupted JSON file {filename}. Skipping.\")\n",
        "#     return processed_ids\n",
        "\n",
        "# # Create LLM prompt for single classification\n",
        "# def create_prompt_for_single_classification(rule, invalid_technique=None):\n",
        "#     \"\"\"\n",
        "#     Generates a prompt for the LLM.\n",
        "#     If retrying, it includes the previously invalid technique and asks the LLM to correct it.\n",
        "#     \"\"\"\n",
        "#     correction_note = \"\"\n",
        "#     if invalid_technique:\n",
        "#         correction_note = f\"\"\"\n",
        "# The previous technique suggestion was INVALID:\n",
        "# - Technique ID: {invalid_technique[\"mitre_technique_id\"]}\n",
        "# - Technique Name: {invalid_technique[\"mitre_technique_name\"]}\n",
        "\n",
        "# Please select a VALID technique from the lates 2023 MITRE ATT&CK framework.\n",
        "# \"\"\"\n",
        "\n",
        "#     return f\"\"\"\n",
        "# Map the following Suricata IDS rule to a **single MITRE ATT&CK technique** with a confidence score (rounded to 2 decimal places).\n",
        "\n",
        "# {correction_note}\n",
        "\n",
        "# ### Rule:\n",
        "# ID: {rule[\"suri_rule_id\"]}\n",
        "# File Name: {rule[\"file_name\"]}\n",
        "# Action: {rule[\"action\"]}\n",
        "# Protocol: {rule[\"protocol\"]}\n",
        "# Source: {rule[\"src_addr\"]}:{rule[\"src_port\"]}\n",
        "# Destination: {rule[\"dst_addr\"]}:{rule[\"dst_port\"]}\n",
        "# Options: {rule[\"options\"]}\n",
        "# Classification: {rule[\"suri_rule_classtype\"]}\n",
        "# Message: \"{rule[\"suri_rule_msg\"]}\"\n",
        "\n",
        "# Respond in valid JSON format:\n",
        "# {{\n",
        "#   \"mitre_technique_id\": \"<Technique ID>\",\n",
        "#   \"mitre_technique_name\": \"<Technique Name>\",\n",
        "#   \"confidence_score\": \"<Confidence Score (rounded to 2 decimal places)>\"\n",
        "# }}\n",
        "# \"\"\".strip()\n",
        "\n",
        "# # Query the LLM\n",
        "# def query_llm(prompt, use_gpt4=False):\n",
        "#     model_name = \"gpt-4\" if use_gpt4 else \"gpt-3.5-turbo\"\n",
        "#     system_message = (\n",
        "#         \"You are a cybersecurity expert. Your task is to map Suricata IDS rules \"\n",
        "#         \"to MITRE ATT&CK techniques, ensuring the technique is valid.\"\n",
        "#     )\n",
        "\n",
        "#     response = client.chat.completions.create(\n",
        "#         model=model_name,\n",
        "#         messages=[\n",
        "#             {\"role\": \"system\", \"content\": system_message},\n",
        "#             {\"role\": \"user\", \"content\": prompt},\n",
        "#         ],\n",
        "#         max_tokens=400,\n",
        "#         temperature=0.0\n",
        "#     )\n",
        "#     return response\n",
        "\n",
        "# # Get nearest embedding match from Pinecone\n",
        "# def get_nearest_mitre_match(technique_name, manager):\n",
        "#     \"\"\"\n",
        "#     Queries Pinecone for the closest matching MITRE technique based on embeddings.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         query = technique_name\n",
        "#         results = manager.retrieve_experiences(query)\n",
        "\n",
        "#         if results:\n",
        "#             top_match = results[0]\n",
        "#             return {\n",
        "#                 \"mitre_technique_id\": top_match.metadata[\"technique_id\"],\n",
        "#                 \"mitre_technique_name\": top_match.metadata[\"technique_name\"],\n",
        "#                 \"confidence_score\": \"0.80\"  # Assign a fixed confidence score for Pinecone match\n",
        "#             }\n",
        "#     except Exception as e:\n",
        "#         print(f\" Error querying Pinecone: {e}\")\n",
        "\n",
        "#     return None  # Return None if no match found\n",
        "\n",
        "# # Validate technique, retry LLM twice with invalid technique info, then fallback to Pinecone\n",
        "# def validate_or_fallback(rule, mapping, manager, use_gpt4=False):\n",
        "#     \"\"\"\n",
        "#     Step 1: Validate the first response.\n",
        "#     Step 2: If invalid, re-query the LLM **twice** with the incorrect technique passed in the prompt.\n",
        "#     Step 3: If still invalid, use Pinecone nearest match.\n",
        "#     \"\"\"\n",
        "#     if mapping[\"mitre_technique_id\"] in MITRE_TECHNIQUES:\n",
        "#         return mapping  # If valid, return immediately.\n",
        "\n",
        "#     print(f\" Invalid MITRE Technique: {mapping['mitre_technique_id']}  Re-querying LLM (Attempt 2)...\")\n",
        "\n",
        "#     # First retry with LLM\n",
        "#     prompt = create_prompt_for_single_classification(rule, invalid_technique=mapping)\n",
        "#     response = query_llm(prompt, use_gpt4=use_gpt4)\n",
        "#     response_text = response.choices[0].message.content.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "#     try:\n",
        "#         second_attempt_mapping = json.loads(response_text)\n",
        "\n",
        "#         # Check second attempt validity\n",
        "#         if second_attempt_mapping[\"mitre_technique_id\"] in MITRE_TECHNIQUES:\n",
        "#             return second_attempt_mapping  # If valid, return it.\n",
        "\n",
        "#     except json.JSONDecodeError:\n",
        "#         print(f\" LLM failed second attempt for rule {rule['suri_rule_id']}.\")\n",
        "\n",
        "#     # Final retry with LLM before using Pinecone\n",
        "#     print(f\" LLM failed twice  Re-querying one last time...\")\n",
        "#     prompt = create_prompt_for_single_classification(rule, invalid_technique=second_attempt_mapping)\n",
        "#     response = query_llm(prompt, use_gpt4=use_gpt4)\n",
        "#     response_text = response.choices[0].message.content.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "#     try:\n",
        "#         third_attempt_mapping = json.loads(response_text)\n",
        "\n",
        "#         if third_attempt_mapping[\"mitre_technique_id\"] in MITRE_TECHNIQUES:\n",
        "#             return third_attempt_mapping  # If valid, return it.\n",
        "\n",
        "#     except json.JSONDecodeError:\n",
        "#         print(f\" LLM failed third attempt for rule {rule['suri_rule_id']}.\")\n",
        "\n",
        "#     # Fallback to Pinecone\n",
        "#     print(f\" LLM failed three times  Using Pinecone for nearest match...\")\n",
        "#     return get_nearest_mitre_match(mapping[\"mitre_technique_name\"], manager)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     from langchain_community.embeddings import OpenAIEmbeddings\n",
        "#     from langchain_community.vectorstores import Pinecone\n",
        "#     from pinecone import Pinecone as PineconeClient\n",
        "\n",
        "#     # Initialize Pinecone Experience Manager\n",
        "#     project_name = \"testautoattackerproject\"\n",
        "#     vectordb_name = \"test_autoattacker_vectordb\"\n",
        "#     manager = ExperienceManager(project_name, vectordb_name)\n",
        "\n",
        "#     process_csv_and_map_to_mitre(\"suricata_extracted_rules_parsed.csv\", manager, use_gpt4=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9Xak5aHbjPbM",
        "outputId": "40c49437-095a-459f-c2f9-eecc4f94156e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Invalid MITRE Technique: T1073  Re-querying LLM (Attempt 2)...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1022  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Processed 50 rules...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " Invalid MITRE Technique: T1024  Re-querying LLM (Attempt 2)...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " Invalid MITRE Technique: T1024  Re-querying LLM (Attempt 2)...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Processed 100 rules...\n",
            " Invalid MITRE Technique: T1022  Re-querying LLM (Attempt 2)...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1024  Re-querying LLM (Attempt 2)...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1193  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Processed 150 rules...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1107  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1116  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " Invalid MITRE Technique: T1116  Re-querying LLM (Attempt 2)...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n",
            " LLM failed twice  Re-querying one last time...\n",
            " LLM failed three times  Using Pinecone for nearest match...\n",
            " Invalid MITRE Technique: T1116  Re-querying LLM (Attempt 2)...\n",
            " Invalid MITRE Technique: T1043  Re-querying LLM (Attempt 2)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-e0625435e101>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperienceManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectordb_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mprocess_csv_and_map_to_mitre\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"suricata_extracted_rules_parsed.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpt4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-edc873d684bc>\u001b[0m in \u001b[0;36mprocess_csv_and_map_to_mitre\u001b[0;34m(csv_input_file, manager, use_gpt4)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalid_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_or_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpt4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalid_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-e0625435e101>\u001b[0m in \u001b[0;36mvalidate_or_fallback\u001b[0;34m(rule, mapping, manager, use_gpt4)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# First retry with LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_prompt_for_single_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_technique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpt4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpt4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0mresponse_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"```json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"```\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-e0625435e101>\u001b[0m in \u001b[0;36mquery_llm\u001b[0;34m(prompt, use_gpt4)\u001b[0m\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    857\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    860\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    997\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import csv\n",
        "import re\n",
        "import openai\n",
        "from google.colab import files  # Google Colab auto-download\n",
        "\n",
        "# OpenAI Client\n",
        "client = openai\n",
        "client.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 2000\n",
        "SAVE_INTERVAL = 1000\n",
        "PROGRESS_UPDATE_INTERVAL = 50\n",
        "START_FROM_RULE = 34001\n",
        "OUTPUT_DIR = \"output_batches\"\n",
        "TECHNIQUES_JSON_FILE = \"techniques.json\"\n",
        "TEST_MODE = False\n",
        "\n",
        "# Ensure output directory exists\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# Load MITRE ATT&CK Techniques (all techniques)\n",
        "def load_mitre_techniques():\n",
        "    with open(TECHNIQUES_JSON_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        techniques = json.load(f)\n",
        "\n",
        "    # Store only technique IDs for quick lookup\n",
        "    technique_dict = {t[\"technique_id\"]: t for t in techniques}\n",
        "    return technique_dict, techniques  # Returning the full list instead of chunks\n",
        "\n",
        "MITRE_TECHNIQUES, ALL_TECHNIQUES = load_mitre_techniques()\n",
        "\n",
        "# Read processed rule IDs\n",
        "def get_processed_rule_ids():\n",
        "    processed_ids = set()\n",
        "    for filename in os.listdir(OUTPUT_DIR):\n",
        "        if filename.endswith(\".json\"):\n",
        "            with open(os.path.join(OUTPUT_DIR, filename), \"r\", encoding=\"utf-8\") as f:\n",
        "                try:\n",
        "                    data = json.load(f)\n",
        "                    processed_ids.update(entry[\"suri_rule_id\"] for entry in data)\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\" Warning: Corrupted JSON file {filename}. Skipping.\")\n",
        "    return processed_ids\n",
        "\n",
        "# Create LLM prompt with all techniques\n",
        "def create_prompt_for_classification(rule, invalid_technique=None):\n",
        "    \"\"\"\n",
        "    Generates a classification prompt with ALL MITRE techniques.\n",
        "    \"\"\"\n",
        "    correction_note = \"\"\n",
        "    if invalid_technique:\n",
        "        correction_note = f\"\"\"\n",
        "The previous technique suggestion was INVALID:\n",
        "- Technique ID: {invalid_technique[\"mitre_technique_id\"]}\n",
        "- Technique Name: {invalid_technique[\"mitre_technique_name\"]}\n",
        "\n",
        "Please select a **valid** technique **ONLY from the list below**.\n",
        "\"\"\"\n",
        "\n",
        "    # Format all techniques for the prompt\n",
        "    technique_list = \"\\n\".join([\n",
        "        f\"- {t['technique_id']} ({t['technique_name']})\" for t in ALL_TECHNIQUES\n",
        "    ])\n",
        "\n",
        "    return f\"\"\"\n",
        "Map the following Suricata IDS rule to a **single MITRE ATT&CK technique** with a confidence score.\n",
        "\n",
        "{correction_note}\n",
        "\n",
        "### Rule:\n",
        "ID: {rule[\"suri_rule_id\"]}\n",
        "File Name: {rule[\"file_name\"]}\n",
        "Classification: {rule[\"suri_rule_classtype\"]}\n",
        "Message: \"{rule[\"suri_rule_msg\"]}\"\n",
        "\n",
        "### **ONLY SELECT FROM THESE TECHNIQUES**:\n",
        "{technique_list}\n",
        "\n",
        "Respond in valid JSON format:\n",
        "{{\n",
        "  \"mitre_technique_id\": \"<Technique ID>\",\n",
        "  \"mitre_technique_name\": \"<Technique Name>\",\n",
        "  \"confidence_score\": \"<Confidence Score (rounded to 2 decimal places)>\"\n",
        "}}\n",
        "\"\"\".strip()\n",
        "\n",
        "# Query the LLM\n",
        "def query_llm(prompt, use_gpt4=False):\n",
        "    model_name = \"gpt-4\" if use_gpt4 else \"gpt-3.5-turbo\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=1000,  # Allow up to 1000 tokens for the response\n",
        "        temperature=0.0\n",
        "    )\n",
        "    return response\n",
        "\n",
        "# Validate, Retry, then Query Pinecone\n",
        "def validate_or_fallback(rule, mapping, manager, use_gpt4=False):\n",
        "    \"\"\"\n",
        "    Step 1: Check if technique is valid.\n",
        "    Step 2: If invalid, retry LLM with ALL techniques.\n",
        "    Step 3: If LLM fails twice, fallback to Pinecone.\n",
        "    \"\"\"\n",
        "    if mapping[\"mitre_technique_id\"] in MITRE_TECHNIQUES:\n",
        "        return mapping  # If valid, return immediately.\n",
        "\n",
        "    print(f\" Invalid MITRE Technique: {mapping['mitre_technique_id']}  Re-querying LLM with ALL techniques...\")\n",
        "\n",
        "    # Retry with ALL techniques included\n",
        "    prompt = create_prompt_for_classification(rule, invalid_technique=mapping)\n",
        "    response = query_llm(prompt, use_gpt4=use_gpt4)\n",
        "    response_text = response.choices[0].message.content.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "    try:\n",
        "        second_attempt_mapping = json.loads(response_text)\n",
        "        if second_attempt_mapping[\"mitre_technique_id\"] in MITRE_TECHNIQUES:\n",
        "            return second_attempt_mapping  # If valid, return it.\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\" LLM failed on second attempt for rule {rule['suri_rule_id']}.\")\n",
        "\n",
        "    # Fallback to Pinecone if LLM fails again\n",
        "    print(f\" LLM failed twice  Using Pinecone for nearest match...\")\n",
        "    return get_nearest_mitre_match(mapping[\"mitre_technique_name\"], manager)\n",
        "\n",
        "# Pinecone Fallback Function\n",
        "def get_nearest_mitre_match(technique_name, manager):\n",
        "    try:\n",
        "        results = manager.retrieve_experiences(technique_name)\n",
        "        if results:\n",
        "            top_match = results[0]\n",
        "            return {\n",
        "                \"mitre_technique_id\": top_match.metadata[\"technique_id\"],\n",
        "                \"mitre_technique_name\": top_match.metadata[\"technique_name\"],\n",
        "                \"confidence_score\": \"0.80\"\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(f\" Error querying Pinecone: {e}\")\n",
        "    return {\n",
        "        \"mitre_technique_id\": \"T9999\",\n",
        "        \"mitre_technique_name\": \"Unknown Technique\",\n",
        "        \"confidence_score\": \"0.00\"\n",
        "    }\n",
        "\n",
        "# Main Processing Pipeline\n",
        "def process_csv_and_map_to_mitre(csv_file, manager, use_gpt4=False):\n",
        "    processed_ids = get_processed_rule_ids()\n",
        "    batch_results = []\n",
        "    total_processed = 0\n",
        "    batch_index = 1\n",
        "\n",
        "    with open(csv_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for i, row in enumerate(reader, start=1):\n",
        "            if i < START_FROM_RULE:\n",
        "                continue\n",
        "\n",
        "            suri_rule_id = f\"{row.get('file_name', 'unknown')}_{i}\"\n",
        "            if suri_rule_id in processed_ids:\n",
        "                continue\n",
        "\n",
        "            rule = {\n",
        "                \"suri_rule_id\": suri_rule_id,\n",
        "                \"file_name\": row.get(\"file_name\", \"N/A\"),\n",
        "                \"action\": row.get(\"action\", \"N/A\"),\n",
        "                \"protocol\": row.get(\"protocol\", \"N/A\"),\n",
        "                \"src_addr\": row.get(\"src_addr\", \"N/A\"),\n",
        "                \"src_port\": row.get(\"src_port\", \"N/A\"),\n",
        "                \"dst_addr\": row.get(\"dst_addr\", \"N/A\"),\n",
        "                \"dst_port\": row.get(\"dst_port\", \"N/A\"),\n",
        "                \"options\": row.get(\"options\", \"\"),\n",
        "                \"suri_rule_classtype\": row.get(\"classtype\", \"N/A\"),\n",
        "                \"suri_rule_msg\": row.get(\"msg\", \"N/A\"),\n",
        "            }\n",
        "\n",
        "            prompt = create_prompt_for_classification(rule)\n",
        "            response = query_llm(prompt, use_gpt4=use_gpt4)\n",
        "            response_text = response.choices[0].message.content.strip()\n",
        "\n",
        "            try:\n",
        "                mapping = json.loads(response_text)\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\" Invalid JSON response for rule {rule['suri_rule_id']}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            corrected_mapping = validate_or_fallback(rule, mapping, manager, use_gpt4=use_gpt4)\n",
        "            batch_results.append({**rule, **corrected_mapping})\n",
        "            total_processed += 1\n",
        "\n",
        "            if total_processed % PROGRESS_UPDATE_INTERVAL == 0:\n",
        "                print(f\" Processed {total_processed} rules...\")\n",
        "\n",
        "            if total_processed % SAVE_INTERVAL == 0:\n",
        "                filename = os.path.join(OUTPUT_DIR, f\"partial_{batch_index}.json\")\n",
        "                with open(filename, \"w\", encoding=\"utf-8\") as f_out:\n",
        "                    json.dump(batch_results, f_out, indent=4)\n",
        "                files.download(filename)\n",
        "                batch_results = []\n",
        "                batch_index += 1\n",
        "\n",
        "    if batch_results:\n",
        "        filename = os.path.join(OUTPUT_DIR, f\"final_{batch_index}.json\")\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f_out:\n",
        "            json.dump(batch_results, f_out, indent=4)\n",
        "        files.download(filename)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual Pinecone manager\n",
        "    process_csv_and_map_to_mitre(\"suricata_extracted_rules_parsed.csv\", manager, use_gpt4=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D6aiYHVztfos",
        "outputId": "d75fa06d-d864-475c-f8fb-a0cdc700e333"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 50 rules...\n",
            " Processed 100 rules...\n",
            " Processed 150 rules...\n",
            " Processed 200 rules...\n",
            " Processed 250 rules...\n",
            " Processed 300 rules...\n",
            " Processed 350 rules...\n",
            " Processed 400 rules...\n",
            " Processed 450 rules...\n",
            " Processed 500 rules...\n",
            " Processed 550 rules...\n",
            " Processed 600 rules...\n",
            " Processed 650 rules...\n",
            " Processed 700 rules...\n",
            " Processed 750 rules...\n",
            " Processed 800 rules...\n",
            " Processed 850 rules...\n",
            " Processed 900 rules...\n",
            " Processed 950 rules...\n",
            " Processed 1000 rules...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_913b4226-1f1f-411d-b82a-11c223c5be0b\", \"partial_1.json\", 1154457)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 1050 rules...\n",
            " Processed 1100 rules...\n",
            " Processed 1150 rules...\n",
            " Processed 1200 rules...\n",
            " Processed 1250 rules...\n",
            " Processed 1300 rules...\n",
            " Processed 1350 rules...\n",
            " Processed 1400 rules...\n",
            " Processed 1450 rules...\n",
            " Processed 1500 rules...\n",
            " Processed 1550 rules...\n",
            " Processed 1600 rules...\n",
            " Processed 1650 rules...\n",
            " Processed 1700 rules...\n",
            " Processed 1750 rules...\n",
            " Processed 1800 rules...\n",
            " Processed 1850 rules...\n",
            " Processed 1900 rules...\n",
            " Processed 1950 rules...\n",
            " Processed 2000 rules...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b52c0ba0-8780-499f-acbe-66d5026df321\", \"partial_2.json\", 945811)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 2050 rules...\n",
            " Processed 2100 rules...\n",
            " Processed 2150 rules...\n",
            " Processed 2200 rules...\n",
            " Processed 2250 rules...\n",
            " Processed 2300 rules...\n",
            " Processed 2350 rules...\n",
            " Processed 2400 rules...\n",
            " Processed 2450 rules...\n",
            " Processed 2500 rules...\n",
            " Processed 2550 rules...\n",
            " Processed 2600 rules...\n",
            " Processed 2650 rules...\n",
            " Processed 2700 rules...\n",
            " Processed 2750 rules...\n",
            " Processed 2800 rules...\n",
            " Processed 2850 rules...\n",
            " Processed 2900 rules...\n",
            " Processed 2950 rules...\n",
            " Processed 3000 rules...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_22e3a6b0-ff28-45af-94de-fd30f380a0bb\", \"partial_3.json\", 986389)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 3050 rules...\n",
            " Processed 3100 rules...\n",
            " Processed 3150 rules...\n",
            " Processed 3200 rules...\n",
            " Processed 3250 rules...\n",
            " Processed 3300 rules...\n",
            " Processed 3350 rules...\n",
            " Processed 3400 rules...\n",
            " Processed 3450 rules...\n",
            " Processed 3500 rules...\n",
            " Processed 3550 rules...\n",
            " Processed 3600 rules...\n",
            " Processed 3650 rules...\n",
            " Processed 3700 rules...\n",
            " Processed 3750 rules...\n",
            " Processed 3800 rules...\n",
            " Processed 3850 rules...\n",
            " Processed 3900 rules...\n",
            " Processed 3950 rules...\n",
            " Processed 4000 rules...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fc4bd0fd-50eb-4f08-a447-b869cffd1dfe\", \"partial_4.json\", 1098448)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 4050 rules...\n",
            " Processed 4100 rules...\n",
            " Processed 4150 rules...\n",
            " Processed 4200 rules...\n",
            " Processed 4250 rules...\n",
            " Processed 4300 rules...\n",
            " Processed 4350 rules...\n",
            " Processed 4400 rules...\n",
            " Processed 4450 rules...\n",
            " Processed 4500 rules...\n",
            " Processed 4550 rules...\n",
            " Processed 4600 rules...\n",
            " Processed 4650 rules...\n",
            " Processed 4700 rules...\n",
            " Processed 4750 rules...\n",
            " Processed 4800 rules...\n",
            " Processed 4850 rules...\n",
            " Processed 4900 rules...\n",
            " Processed 4950 rules...\n",
            " Processed 5000 rules...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e448a7e9-35fd-426e-9902-4b15aeb3de8c\", \"partial_5.json\", 1230660)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 5050 rules...\n",
            " Processed 5100 rules...\n",
            " Processed 5150 rules...\n",
            " Processed 5200 rules...\n",
            " Processed 5250 rules...\n",
            " Processed 5300 rules...\n",
            " Processed 5350 rules...\n",
            " Processed 5400 rules...\n",
            " Processed 5450 rules...\n",
            " Processed 5500 rules...\n",
            " Processed 5550 rules...\n",
            " Processed 5600 rules...\n",
            " Processed 5650 rules...\n",
            " Processed 5700 rules...\n",
            " Processed 5750 rules...\n",
            " Processed 5800 rules...\n",
            " Processed 5850 rules...\n",
            " Processed 5900 rules...\n",
            " Processed 5950 rules...\n",
            " Processed 6000 rules...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2d2cc339-457e-4cc3-884c-c41c845f16f7\", \"partial_6.json\", 1182964)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 6050 rules...\n",
            " Processed 6100 rules...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bd84868c-b67a-41e6-b8d6-4f52cf71ccaa\", \"final_7.json\", 133879)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# # Load JSON file\n",
        "# data = pd.read_json(\"testDataMapped.json\")\n",
        "# # Save to CSV\n",
        "# data.to_csv(\"testDataMapped.csv\", index=False)"
      ],
      "metadata": {
        "id": "j0suf8zdYkYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Directory that holds your exec_*.json files\n",
        "INPUT_DIR = \"output_batches_final\"  # or wherever your files are\n",
        "# Final merged output filename\n",
        "OUTPUT_FILE = \"all_mapped_results.json\"\n",
        "\n",
        "def merge_json_files():\n",
        "    # We'll store all results here\n",
        "    merged_data = []\n",
        "\n",
        "    # Keep track of totals\n",
        "    files_processed = 0\n",
        "    rules_loaded = 0\n",
        "\n",
        "    # For convenience, list all files named exec_*.json\n",
        "    # Adjust the pattern if your files are named differently.\n",
        "    json_files = [f for f in os.listdir(INPUT_DIR) if f.startswith(\"exec_\") and f.endswith(\".json\")]\n",
        "\n",
        "    # Sort them in ascending order so 'exec_0.json' comes before 'exec_41.json'\n",
        "    json_files.sort()\n",
        "\n",
        "    for filename in json_files:\n",
        "        file_path = os.path.join(INPUT_DIR, filename)\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "                if isinstance(data, list):\n",
        "                    merged_data.extend(data)\n",
        "                    print(f\" Merged {filename} with {len(data)} items.\")\n",
        "                    rules_loaded += len(data)\n",
        "                else:\n",
        "                    # If not a list, decide how to handle.\n",
        "                    # Possibly wrap it or just append a single object.\n",
        "                    merged_data.append(data)\n",
        "                    print(f\" Merged {filename} with 1 item (not a list).\")\n",
        "                    rules_loaded += 1\n",
        "\n",
        "                files_processed += 1\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\" Skipping file {filename} - invalid JSON.\")\n",
        "        except Exception as e:\n",
        "            print(f\" Error reading {filename}: {e}\")\n",
        "\n",
        "    # Write out the combined data\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out:\n",
        "        json.dump(merged_data, out, indent=4)\n",
        "\n",
        "    # Final logging\n",
        "    print(f\"\\n Successfully created merged file: {OUTPUT_FILE}\")\n",
        "    print(f\"    Total files processed: {files_processed}\")\n",
        "    print(f\"    Total rules loaded: {rules_loaded}\")\n",
        "    print(f\"    Total items in merged file: {len(merged_data)}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    merge_json_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe_8qUCS70hK",
        "outputId": "2e27495a-271f-42fe-aa0d-49b06b155150"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Merged exec_0.json with 1000 items.\n",
            " Merged exec_1.json with 1000 items.\n",
            " Merged exec_10.json with 1000 items.\n",
            " Merged exec_11.json with 1000 items.\n",
            " Merged exec_12.json with 1000 items.\n",
            " Merged exec_13.json with 1000 items.\n",
            " Merged exec_14.json with 1000 items.\n",
            " Merged exec_15.json with 1000 items.\n",
            " Merged exec_16.json with 1000 items.\n",
            " Merged exec_17.json with 1000 items.\n",
            " Merged exec_18.json with 1000 items.\n",
            " Merged exec_19.json with 1000 items.\n",
            " Merged exec_2.json with 1000 items.\n",
            " Merged exec_20.json with 1000 items.\n",
            " Merged exec_21.json with 1000 items.\n",
            " Merged exec_22.json with 1000 items.\n",
            " Merged exec_23.json with 1000 items.\n",
            " Merged exec_24.json with 1000 items.\n",
            " Merged exec_25.json with 1000 items.\n",
            " Merged exec_26.json with 1000 items.\n",
            " Merged exec_27.json with 1000 items.\n",
            " Merged exec_28.json with 1000 items.\n",
            " Merged exec_29.json with 1000 items.\n",
            " Merged exec_3.json with 1000 items.\n",
            " Merged exec_30.json with 1000 items.\n",
            " Merged exec_31.json with 1000 items.\n",
            " Merged exec_32.json with 1000 items.\n",
            " Merged exec_33.json with 1000 items.\n",
            " Merged exec_34.json with 1000 items.\n",
            " Merged exec_35.json with 1000 items.\n",
            " Merged exec_36.json with 1000 items.\n",
            " Merged exec_37.json with 1000 items.\n",
            " Merged exec_38.json with 1000 items.\n",
            " Merged exec_39.json with 1000 items.\n",
            " Merged exec_4.json with 1000 items.\n",
            " Merged exec_40.json with 144 items.\n",
            " Merged exec_5.json with 1000 items.\n",
            " Merged exec_6.json with 1000 items.\n",
            " Merged exec_7.json with 1000 items.\n",
            " Merged exec_8.json with 1000 items.\n",
            " Merged exec_9.json with 1000 items.\n",
            "\n",
            " Successfully created merged file: all_mapped_results.json\n",
            "    Total files processed: 41\n",
            "    Total rules loaded: 40144\n",
            "    Total items in merged file: 40144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "previous code"
      ],
      "metadata": {
        "id": "ED_vYeHJQkWw"
      }
    }
  ]
}